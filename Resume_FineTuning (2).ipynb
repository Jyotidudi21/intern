{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wXQJEfNlJoRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f0e5b5-fde7-4f60-dd81-642abce1eb0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126718 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.11_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.11) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.11) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq poppler-utils tesseract-ocr\n",
        "\n",
        "!pip install -q transformers datasets accelerate evaluate sentencepiece pdfplumber PyMuPDF pytesseract pillow sentence-transformers spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "DATA_DIR = \"/content/drive/MyDrive/resume_data\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/resume_qa_Model\"\n",
        "TEXT_DIR = \"/content/resume_texts\"\n",
        "os.makedirs(TEXT_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"MODEL_DIR:\", MODEL_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuTnUP6gZFKM",
        "outputId": "976f8562-6cf8-40dc-ea44-ce6d9527b017"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "DATA_DIR: /content/drive/MyDrive/resume_data\n",
            "MODEL_DIR: /content/drive/MyDrive/resume_qa_Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber, fitz, pytesseract\n",
        "from PIL import Image\n",
        "import io, re\n",
        "\n",
        "def clean_text(t: str) -> str:\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    t = re.sub(r'\\r\\n?', '\\n', t)\n",
        "    t = re.sub(r'\\n{2,}', '\\n\\n', t)\n",
        "    t = re.sub(r'[ \\t]+', ' ', t)\n",
        "    return t.strip()\n",
        "\n",
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Tries multiple extractors in order: pdfplumber -> PyMuPDF -> Tesseract OCR\n",
        "    Returns cleaned text.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    # pdfplumber\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for p in pdf.pages:\n",
        "                t = p.extract_text()\n",
        "                if t:\n",
        "                    text += t + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(\"pdfplumber error:\", e)\n",
        "\n",
        "    #  PyMuPDF\n",
        "    if len(text.split()) < 50:\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            text = \"\"\n",
        "            for p in doc:\n",
        "                text += p.get_text(\"text\") + \"\\n\"\n",
        "        except Exception as e:\n",
        "            print(\"PyMuPDF error:\", e)\n",
        "\n",
        "    # OCR\n",
        "    if len(text.split()) < 50:\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            ocr_text = \"\"\n",
        "            for page in doc:\n",
        "                pix = page.get_pixmap()\n",
        "                img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
        "                ocr_text += pytesseract.image_to_string(img) + \"\\n\"\n",
        "            text = ocr_text\n",
        "        except Exception as e:\n",
        "            print(\"Tesseract OCR error:\", e)\n",
        "\n",
        "    return clean_text(text)\n"
      ],
      "metadata": {
        "id": "JVWk00uuZZCY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "pdf_paths = sorted([p for p in glob.glob(os.path.join(DATA_DIR, \"*.pdf\"))])\n",
        "print(\"Found PDFs:\", len(pdf_paths))\n",
        "\n",
        "for p in pdf_paths:\n",
        "    fname = os.path.basename(p)\n",
        "    out_txt = os.path.join(TEXT_DIR, fname.replace(\".pdf\", \".txt\"))\n",
        "    if os.path.exists(out_txt):\n",
        "        print(\"Skipping (exists):\", out_txt)\n",
        "        continue\n",
        "    txt = extract_text_from_pdf(p)\n",
        "    if txt and len(txt.split())>20:\n",
        "        with open(out_txt, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(txt)\n",
        "        print(\"Saved:\", out_txt)\n",
        "    else:\n",
        "        print(\"Extraction produced too little text for:\", fname)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UHXU3NtZoYR",
        "outputId": "df4f5f16-60e9-432f-e4a3-e8072db1c789"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found PDFs: 16\n",
            "Saved: /content/resume_texts/17045017_Aniruddha_Sharma_Resume.txt\n",
            "Saved: /content/resume_texts/20045029_Ayushi_Yadav _INTERVIEW.txt\n",
            "Saved: /content/resume_texts/21035033_Kana _Yadav_Maths (10).txt\n",
            "Saved: /content/resume_texts/21035036_Krishna_Tripathi_KRT_web.txt\n",
            "Saved: /content/resume_texts/21045065_Jyoti__BA (10).txt\n",
            "Saved: /content/resume_texts/Ajitesh _Pandey_Resume_IITBHU (1).txt\n",
            "Saved: /content/resume_texts/Anurag_Tripathi_CV.txt\n",
            "Saved: /content/resume_texts/JYOTI_resume (4).txt\n",
            "Saved: /content/resume_texts/KUMAR SOURAV - RESUME (2)_compressed (2).txt\n",
            "Saved: /content/resume_texts/Lipika-Chaudhary-sde (1).txt\n",
            "Saved: /content/resume_texts/Off_Campus_Resume Akansha Upadhyay .txt\n",
            "Saved: /content/resume_texts/Prithivi_R.txt\n",
            "Saved: /content/resume_texts/Resume202506300110.txt\n",
            "Saved: /content/resume_texts/Resume_offcampus.txt\n",
            "Saved: /content/resume_texts/h.txt\n",
            "Saved: /content/resume_texts/personal_resume-1-2-1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  auto-generates simple QA pairs using regex and heuristics.\n",
        "#  fine-tuning and we  label a proper SQuAD dataset manually or augment the output.\n",
        "\n",
        "import json, re\n",
        "from pathlib import Path\n",
        "\n",
        "def auto_generate_qas(text):\n",
        "    qas = []\n",
        "    # email\n",
        "    em = re.findall(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', text)\n",
        "    if em:\n",
        "        qas.append({\"question\":\"What is the candidate's email address?\", \"answers\":[{\"text\": em[0], \"answer_start\": text.find(em[0])}], \"id\":\"email\"})\n",
        "\n",
        "    # phone\n",
        "    ph = re.findall(r'(\\+?\\d[\\d\\s\\-\\(\\)]{7,}\\d)', text)\n",
        "    if ph:\n",
        "        qas.append({\"question\":\"What is the candidate's phone number?\", \"answers\":[{\"text\": ph[0], \"answer_start\": text.find(ph[0])}], \"id\":\"phone\"})\n",
        "\n",
        "\n",
        "    first_block = text.strip().split(\"\\n\")[0:2]\n",
        "    cand = \" \".join(first_block).strip()\n",
        "\n",
        "    nm = re.search(r'([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)', cand)\n",
        "    if nm:\n",
        "        qas.append({\"question\":\"What is the candidate's full name?\", \"answers\":[{\"text\": nm.group(0), \"answer_start\": text.find(nm.group(0))}], \"id\":\"name\"})\n",
        "\n",
        "    # skills line heuristic\n",
        "    skills = re.search(r'(Skills|Technical Skills|Skills:)\\s*[:\\-]?\\s*(.+)', text, re.IGNORECASE)\n",
        "    if skills:\n",
        "        s = skills.group(2).split('\\n')[0].strip()\n",
        "        qas.append({\"question\":\"What are the candidate's skills?\", \"answers\":[{\"text\": s, \"answer_start\": text.find(s)}], \"id\":\"skills\"})\n",
        "\n",
        "    # education heuristic to find degree keywords\n",
        "    edu = re.search(r'(B\\.?A\\.?|B\\.?Sc|B\\.?E\\.|M\\.?A|M\\.?S|M\\.?Tech|MBA|Ph\\.?D)[^\\n]{0,120}', text, re.IGNORECASE)\n",
        "    if edu:\n",
        "        ed = edu.group(0).strip()\n",
        "        qas.append({\"question\":\"What is the candidate's education?\", \"answers\":[{\"text\": ed, \"answer_start\": text.find(ed)}], \"id\":\"education\"})\n",
        "\n",
        "    return qas\n",
        "\n",
        "dataset = {\"data\": []}\n",
        "for txt_file in sorted(Path(TEXT_DIR).glob(\"*.txt\")):\n",
        "    txt = txt_file.read_text(encoding=\"utf-8\")\n",
        "    qas = auto_generate_qas(txt)\n",
        "    if qas:\n",
        "        dataset[\"data\"].append({\"title\": txt_file.name, \"paragraphs\": [{\"context\": txt, \"qas\": qas}]})\n",
        "\n",
        "out_path = \"/content/resume_squad_auto.json\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(dataset, f, indent=2)\n",
        "print(\"Saved auto-generated SQuAD JSON to:\", out_path)\n",
        "print(\"Number of examples:\", len(dataset[\"data\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9LJgHROamk0",
        "outputId": "bec84650-b7bc-4d86-b121-ff6e7c9ac503"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved auto-generated SQuAD JSON to: /content/resume_squad_auto.json\n",
            "Number of examples: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "j = json.load(open(\"/content/resume_squad_auto.json\", \"r\", encoding=\"utf-8\"))\n",
        "print(\"Examples:\", len(j[\"data\"]))\n",
        "if j[\"data\"]:\n",
        "    print(\"Sample title:\", j[\"data\"][0][\"title\"])\n",
        "    print(\"Sample QAs:\", j[\"data\"][0][\"paragraphs\"][0][\"qas\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IScHk8HxbtJa",
        "outputId": "10ced89f-85d3-474a-83a9-2c73547137fa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples: 16\n",
            "Sample title: 17045017_Aniruddha_Sharma_Resume.txt\n",
            "Sample QAs: [{'question': \"What is the candidate's email address?\", 'answers': [{'text': 'aniruddha.sharma0019@gmail.com', 'answer_start': 3573}], 'id': 'email'}, {'question': \"What is the candidate's phone number?\", 'answers': [{'text': '9589339000', 'answer_start': 3559}], 'id': 'phone'}, {'question': \"What are the candidate's skills?\", 'answers': [{'text': 'Technical Skills Python, SQL, MS-Excel, Tableau', 'answer_start': 259}], 'id': 'skills'}, {'question': \"What is the candidate's education?\", 'answers': [{'text': 'MA', 'answer_start': 14}], 'id': 'education'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/resume_qa_Model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGwRTsb39iyp",
        "outputId": "d917cee6-c181-4338-ce75-2c6928e74282"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint-228\tmerges.txt\t   special_tokens_map.json  tokenizer.json\n",
            "config.json\tmodel.safetensors  tokenizer_config.json    vocab.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/resume_qa_Model\"\n",
        "\n",
        "\n",
        "qa_ft = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=MODEL_DIR,\n",
        "    tokenizer=MODEL_DIR,\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")\n",
        "\n",
        "\n",
        "TEXT_DIR = \"/content/resume_texts\"\n",
        "sample_txts = sorted([p for p in Path(TEXT_DIR).glob(\"*.txt\")])\n",
        "\n",
        "if sample_txts:\n",
        "    ctx = sample_txts[0].read_text(encoding=\"utf-8\")\n",
        "    q = \"What is the candidate's email address?\"\n",
        "    print(\"Question:\", q)\n",
        "    answer = qa_ft(question=q, context=ctx)\n",
        "    print(\"\\nAnswer:\", answer)\n",
        "else:\n",
        "    print(\" No sample text found to test.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfZPBMXH76Tv",
        "outputId": "c00f4cda-80a5-48a4-f737-01555163e2b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the candidate's email address?\n",
            "\n",
            "Answer: {'score': 1.9964377749674895, 'start': 3573, 'end': 3603, 'answer': 'aniruddha.sharma0019@gmail.com'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zG5hs31A9RnH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}